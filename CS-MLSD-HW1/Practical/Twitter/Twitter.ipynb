{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f089f7cf",
   "metadata": {},
   "source": [
    "# Emotion Detection in Persian Texts\n",
    "\n",
    "**Student Name:** <span style=\"color:cyan\">Ardalan Siavashpour</span>\n",
    "\n",
    "**Student ID:** <span style=\"color:cyan\">99109896</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af237db",
   "metadata": {},
   "source": [
    "## 1. Project Overview and Goals\n",
    "\n",
    "In this assignment, you will build a machine learning pipeline to detect emotions in a single-label collection of Persian texts. The dataset is categorized into five emotional classes: **HAPPY, SAD, ANGRY, FEAR, and OTHER**.\n",
    "\n",
    "### Your Tasks:\n",
    "\n",
    "*   **Data Cleaning & Feature Engineering:** Preprocess the Persian text data.\n",
    "*   **Model Selection:** Choose a suitable classical machine learning model.\n",
    "*   **Pipeline Construction:** Use the provided `EmotionClassifierPipeline` class to encapsulate your workflow.\n",
    "*   **Model Evaluation:** Use K-Fold/Stratified K-Fold cross-validation to evaluate your model's performance and interpret the results.\n",
    "*   **Prediction:** Train your final pipeline on the entire training dataset and generate predictions for the unlabeled test set.\n",
    "*   **Submission:** Save your test predictions to a CSV file named `submission.csv`.\n",
    "\n",
    "**Grading:** Achieving an accuracy level above **65%** on the hidden test set will result in full marks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be18fb4d",
   "metadata": {},
   "source": [
    "## 2. Setup and Data Loading\n",
    "\n",
    "First, let's import the necessary libraries and load our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d46edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ardalan-sia/Documents/ML/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ardalan-sia/Documents/ML/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m762.5 kB/s\u001b[0m  \u001b[33m0:00:16\u001b[0ma \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m2\u001b[0m0m\n",
      "\u001b[?25hUsing cached numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Using cached scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, numpy, joblib, et-xmlfile, scipy, pandas, openpyxl, scikit-learn\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [scikit-learn] \u001b[32m 9/10\u001b[0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed et-xmlfile-2.0.0 joblib-1.5.2 numpy-2.3.5 openpyxl-3.1.5 pandas-2.3.3 pytz-2025.2 scikit-learn-1.7.2 scipy-1.16.3 threadpoolctl-3.6.0 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda8ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Scikit-learn modules for machine learning\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the Datasets ---\n",
    "# Make sure the files 'HW1P1_train.xlsx' and 'HW1P1_test.csv' are in the same directory.\n",
    "df_train = pd.read_excel('Twitter_train.xlsx')\n",
    "df_test = pd.read_csv('Twitter_test.csv')\n",
    "\n",
    "print(\"--- Training Data Head ---\")\n",
    "display(df_train.head())\n",
    "\n",
    "print(\"\\n--- Test Data Head ---\")\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327fd1c2",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA) and Preprocessing\n",
    "\n",
    "### 3.1. Analyze Class Distribution\n",
    "\n",
    "**TODO:** Analyze and visualize the distribution of emotions in the training set. Is the dataset balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b09e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE to analyze and plot the emotion distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb950b5",
   "metadata": {},
   "source": [
    "### 3.2. Text Cleaning and Preprocessing\n",
    "\n",
    "**TODO:** Implement a function to clean the Persian text. Consider steps like normalization, removing punctuation and numbers, and handling stop words. Apply this function to create a new `cleaned_text` column in both `df_train` and `df_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_persian_text(text):\n",
    "    \"\"\"\n",
    "    A function to clean and preprocess Persian text.\n",
    "    \n",
    "    TODO: Implement your text cleaning logic here.\n",
    "    \"\"\"\n",
    "    \n",
    "    pass # Replace this with your code\n",
    "\n",
    "# --- Apply your cleaning function ---\n",
    "# TODO: Run these lines after you have implemented your function.\n",
    "df_train['cleaned_text'] = df_train['text'].apply(clean_persian_text)\n",
    "df_test['cleaned_text'] = df_test['text'].apply(clean_persian_text)\n",
    "\n",
    "print(\"Text cleaning complete. Example:\")\n",
    "display(df_train[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e79ff1",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation with Cross-Validation\n",
    "\n",
    "**TODO:** Use (Stratified) K-Fold to evaluate different models and vectorizers. Your goal is to find the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad138618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data (ensure you have run the cleaning step above)\n",
    "X = df_train['cleaned_text']\n",
    "y = df_train['emotion']\n",
    "\n",
    "# --- Define your components ---\n",
    "# TODO: Experiment to find the best components for your pipeline.\n",
    "\n",
    "# --- Cross-Validation Loop ---\n",
    "N_SPLITS = 5\n",
    "skf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "fold_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} ---\")\n",
    "    \n",
    "#     # TODO: Split data into training and validation sets for this fold\n",
    "#     # YOUR CODE HERE\n",
    "    \n",
    "#     # TODO: Instantiate your pipeline for this fold\n",
    "#     # YOUR CODE HERE\n",
    "    \n",
    "#     # TODO: Fit the pipeline on the training fold\n",
    "#     # YOUR CODE HERE\n",
    "    \n",
    "#     # TODO: Make predictions on the validation fold\n",
    "#     # YOUR CODE HERE\n",
    "    \n",
    "#     # TODO: Calculate and store appropriate metrics\n",
    "#     # YOUR CODE HERE\n",
    "\n",
    "# --- Print Average Scores ---\n",
    "print(\"\\n--- Cross-Validation Summary ---\")\n",
    "print(f\"Average Accuracy: {np.mean(fold_accuracies):.4f} (+/- {np.std(fold_accuracies):.4f})\")\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f50d4e3",
   "metadata": {},
   "source": [
    "## 5. Building the Machine Learning Pipeline\n",
    "\n",
    "Here is a custom pipeline class provided for you. Your task is put your chosen components inside this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79276982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Based on your experiments in Section 5, define the final\n",
    "# pipeline you will use.\n",
    "class EmotionClassifierPipeline:\n",
    "    \"\"\"\n",
    "    A custom pipeline class to handle text vectorization and classification.\n",
    "    This class is provided for you to use.\n",
    "    \"\"\"\n",
    "    def __init__(self, vectorizer, classifier):\n",
    "        \"\"\"\n",
    "        Initializes the pipeline with chosen components.\n",
    "        \"\"\"\n",
    "        self.vectorizer = vectorizer\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains the pipeline on the provided training data.\n",
    "        \"\"\"\n",
    "        X_transformed = self.vectorizer.fit_transform(X)\n",
    "        self.classifier.fit(X_transformed, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts labels for new, unseen data.\n",
    "        \"\"\"\n",
    "        X_transformed = self.vectorizer.transform(X)\n",
    "        predictions = self.classifier.predict(X_transformed)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b0fb64",
   "metadata": {},
   "source": [
    "## 6. Final Model Training and Prediction\n",
    "\n",
    "Now, you will train your best-performing pipeline on the **entire training dataset** and generate predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b5aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reload the data to ensure we are working with the original sets ---\n",
    "print(\"Reloading data for final training and prediction...\")\n",
    "df_train = pd.read_excel('train.xlsx')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# --- Apply the SAME cleaning process ---\n",
    "# TODO: Make sure your 'clean_persian_text' function is defined above.\n",
    "# Then, apply it here.\n",
    "df_train['cleaned_text'] = df_train['text'].apply(clean_persian_text)\n",
    "df_test['cleaned_text'] = df_test['text'].apply(clean_persian_text)\n",
    "\n",
    "\n",
    "# --- Instantiate and Fit the Final Pipeline ---\n",
    "print(\"Training the final pipeline on all training data...\")\n",
    "\n",
    "# This part uses the provided pipeline class to train your chosen model\n",
    "final_pipeline = EmotionClassifierPipeline(\n",
    "    vectorizer=final_vectorizer,\n",
    "    classifier=final_classifier\n",
    ")\n",
    "\n",
    "# Fit the pipeline on the full, cleaned training data\n",
    "# final_pipeline.fit(df_train['cleaned_text'], df_train['emotion'])\n",
    "\n",
    "print(\"Final pipeline training complete.\")\n",
    "\n",
    "\n",
    "# --- Generate Predictions on the Test Set ---\n",
    "print(\"Generating predictions for the test set...\")\n",
    "\n",
    "# Predict using the cleaned test data\n",
    "test_predictions = final_pipeline.predict(df_test['cleaned_text'])\n",
    "\n",
    "# Add predictions to the test DataFrame\n",
    "df_test['predicted_emotion'] = test_predictions\n",
    "\n",
    "print(\"\\n--- Test Data with Predictions (Top 5) ---\")\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f9c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create the submission DataFrame ---\n",
    "submission_df = pd.DataFrame({\n",
    "    'text': df_test['text'],\n",
    "    'emotion': df_test['predicted_emotion']\n",
    "})\n",
    "\n",
    "# --- Save to CSV ---\n",
    "output_filename = 'submission.csv'\n",
    "submission_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully saved predictions to '{output_filename}'.\")\n",
    "display(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
